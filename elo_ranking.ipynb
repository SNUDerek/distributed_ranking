{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# demonstration of distributed ranking/sorting with Elo\n",
    "\n",
    "simulation of a distributed ranking system for a large number of items using a number of unreliable users, and rewarding users with reputation\n",
    "\n",
    "*The Social Network* references the 'chess ranking algorithm' Elo rating as the sorting mechanism of Facemash\n",
    "\n",
    "this was motivated by the following post:\n",
    "\n",
    "https://stackoverflow.com/questions/164831/how-to-rank-a-million-images-with-a-crowdsourced-sort\n",
    "\n",
    "### The Elo System\n",
    "\n",
    "https://www.geeksforgeeks.org/elo-rating-algorithm/\n",
    "\n",
    "1. new players start with rating of 1600\n",
    "2. win_probability = 1/(1 + 10^((opponent_rating – player_rating)/400))\n",
    "3. score_point = 1 point if they win the match, 0 if they lose, and 0.5 for a draw.\n",
    "4. player_new_rating = player_rating + (K_value * (score_point – win_probability))\n",
    "\n",
    "### Features\n",
    "\n",
    "- distributed: nothing depends on caching information; every choice is made at the time of choosing\n",
    "- asynchronous: while not demonstrated here, there is no time-dependent order to ranking\n",
    "- dual-ranking: maintains sample score and reputation score together\n",
    "\n",
    "### Rejected Alternatives\n",
    "\n",
    "Bubble or Merge Sort: relies on users to respond in timely fashion, as these are sequenced  \n",
    "Majority-Wins Reputation (like Papago Gym): relies on caching results over time period  \n",
    "Swiss Tournament Pairing: relies on caching pairs; can't add new samples until one session is over  \n",
    "\n",
    "### Parameters\n",
    "\n",
    "`getpairing() n` : `n` controls the number of lowest-seen items to select from  \n",
    "`Crowdscorer noise` : the *un*reliability of each user, as a list (where unreliablility = prob of choosing wrong)  \n",
    "`Crowdscorer reward` : how much to adjust reputations up/down based on expected score (linear)  \n",
    "`Crowdscorer temperature` : adjust how flat the probability sampling is over the users (higher = flatter)  \n",
    "`updateELO() k` : how 'swingy' is the ELO adjustment. higher = bigger adjustments. for chess, k=32 or k=20 is normal  \n",
    "`crowdscoring() users` : controls how many users are grading (related to `Crowdscoreer noise`)\n",
    "\n",
    "### Todo\n",
    "\n",
    "adjust reputation adjustment; right now the score is never normalized so it just keeps going higher. marginal reputation update could be inversely proportional to total number of samples judged, or something... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task: sorting integers\n",
    "\n",
    "for this toy example, we will represent each sample as an integer, with the goal being to choose the higher integer between two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10000, 10000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a random sequence of integers\n",
    "num_samples = 10000\n",
    "max_value   = 50000\n",
    "\n",
    "# initialize data to random values\n",
    "data = np.random.randint(1, max_value, num_samples)\n",
    "# initialize elo scores to 1600 +/- 100\n",
    "elos = [1600 + np.random.randint(-100, 100) for _ in range(num_samples)]\n",
    "# initialize # of validations to 0\n",
    "hits = [0 for _ in range(num_samples)]\n",
    "# check lens\n",
    "len(data), len(elos), len(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn to rank two lists by one list\n",
    "def rank_x_by_y(a, b):\n",
    "    results = list(zip(a, b))\n",
    "    results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "    srt_a = [t[0] for t in results]\n",
    "    srt_b = [t[1] for t in results]\n",
    "    return srt_a, srt_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initial rankings are random\n",
    "\n",
    "here we check the initial distribution by ranking according to the randomized initial Elo scores.\n",
    "\n",
    "we can see that the distribution is random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top ranked:\n",
      "elo \t value\n",
      "1699 \t  20001\n",
      "1699 \t  25079\n",
      "1699 \t  49798\n",
      "1699 \t  27254\n",
      "1699 \t  40051\n",
      "1699 \t  20652\n",
      "1699 \t  7189\n",
      "1699 \t  44127\n",
      "1699 \t  27900\n",
      "1699 \t  8214\n",
      "\n",
      "lowest ranked:\n",
      "elo \t value\n",
      "1500 \t  49175\n",
      "1500 \t  30415\n",
      "1500 \t  48409\n",
      "1500 \t  27463\n",
      "1500 \t  7299\n",
      "1500 \t  6507\n",
      "1500 \t  15158\n",
      "1500 \t  23928\n",
      "1500 \t  8337\n",
      "1500 \t  49828\n"
     ]
    }
   ],
   "source": [
    "# top scores\n",
    "init_data, init_elos = rank_x_by_y(data, elos)\n",
    "print(\"top ranked:\")\n",
    "print(\"elo\", '\\t', \"value\")\n",
    "for i in range(10):\n",
    "    print(init_elos[i], '\\t', '{: 5.0f}'.format(init_data[i]))\n",
    "print(\"\\nlowest ranked:\")\n",
    "print(\"elo\", '\\t', \"value\")\n",
    "for i in reversed(range(10)):\n",
    "    print(init_elos[-i-1], '\\t', '{: 5.0f}'.format(init_data[-i-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iterative scoring\n",
    "\n",
    "while scoring:\n",
    "1. choose (*similarly-ranked*) sents from the less-seen ones\n",
    "2. display to \"user\" who chooses best, with noise (sometimes wrong)\n",
    "3. update Elo and hit count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose pair helper function\n",
    "# this returns INDICES (not values) of pair\n",
    "# method: choose randomly from the n least-seen items, then get closest scrore\n",
    "def getpairing(hitcounts, eloscores, n=100):\n",
    "    # get n indices of lowest-seen\n",
    "    low = np.argpartition(hitcounts, n)[:n]\n",
    "    # sample random index\n",
    "    idx = np.random.choice(low, 1, replace=False)[0]\n",
    "    # get score differences\n",
    "    elodf = np.ma.array(np.abs(np.array(eloscores)[low]-np.array(eloscores)[idx]), mask=False)\n",
    "    elodf.mask[low.tolist().index(idx)] = True\n",
    "    bdx = low[np.argmin(elodf)]\n",
    "    # return\n",
    "    # return idx[0], idx[1]\n",
    "    return idx, bdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6679 1648 6688 1648\n",
      "6646 1637 6701 1638\n",
      "6582 1501 6662 1501\n",
      "6666 1584 6710 1582\n",
      "6704 1572 6663 1573\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "for i in range(5):\n",
    "    a, b = getpairing(hits, elos)\n",
    "    print(a, elos[a], b, elos[b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simulate a crowd of unreliable scorers\n",
    "\n",
    "initialize a number of users, specified by their reliability (noise = % that they guess wrong)\n",
    "\n",
    "when scoring, assign to a user based on their reputation\n",
    "\n",
    "reward/penalize user reputations based on *expected* result == better current Elo score\n",
    "\n",
    "reward policy is linear (don't scale up/down according to # of times that the users have been chosen)\n",
    "\n",
    "temperature is softmax temperature for prob dist, bigger = flatter\n",
    "\n",
    "hypothesis: this correlated with true reliability (1.0 - noise) ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose best with noise function\n",
    "# this should be VALUES (not indices of data)\n",
    "# noise is the % of unreliability\n",
    "def choosebest(v1, v2, noise=0.25):\n",
    "    choices = [v1, v2]\n",
    "    wrong = (noise > np.random.random())\n",
    "    if not wrong:\n",
    "        return choices.index(max(choices))\n",
    "    else:\n",
    "        return choices.index(min(choices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6583 6641 1 True\n",
      "6659 6682 1 True\n",
      "6584 6694 0 False\n",
      "6569 6676 1 True\n",
      "6591 6704 1 True\n",
      "6715 6693 0 True\n",
      "6705 6693 0 True\n",
      "6656 6597 1 False\n",
      "6585 6600 1 True\n",
      "6587 6665 1 True\n"
     ]
    }
   ],
   "source": [
    "# test the default 25% - should be wrong about 2~3 times on average\n",
    "for i in range(10):\n",
    "    v1, v2 = getpairing(hits, elos) # < this is 'wrong' for real task, just testing\n",
    "    idx = choosebest(v1, v2)\n",
    "    choices = [v1, v2]\n",
    "    if choices.index(max(choices)) == idx:\n",
    "        tru = \"True\"\n",
    "    else:\n",
    "        tru = \"False\"\n",
    "    print(v1, v2, idx, tru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group rescorer class\n",
    "# this simulates a number of graders\n",
    "# each grader has their own \"reputation\" using the same ELO system\n",
    "class Crowdscorer:\n",
    "    \n",
    "    def __init__(self, noises, reward=0.01, temperature=10):\n",
    "        self.noises=noises\n",
    "        self.reward=reward\n",
    "        self.temp=temperature\n",
    "        self.reputations=[1.0 for _ in range(len(noises))]\n",
    "        self.selected=[0 for _ in range(len(noises))]\n",
    "        \n",
    "    def _choosebest(self, v1, v2, noise=0.0):\n",
    "        choices = [v1, v2]\n",
    "        wrong = (noise > np.random.random())\n",
    "        if not wrong:\n",
    "            return choices.index(max(choices))\n",
    "        else:\n",
    "            return choices.index(min(choices))\n",
    "        \n",
    "    def score(self, v1, v2):\n",
    "        # get 'true' based on prior\n",
    "        prior = self._choosebest(v1, v2, noise=0.0)\n",
    "        # get sampling proabability based on reputation\n",
    "        e_x = np.exp(np.array(self.reputations)/self.temp)\n",
    "        prb = e_x/e_x.sum()\n",
    "        # select user\n",
    "        idx = np.random.choice([i for i in range(len(self.noises))], p=prb)\n",
    "        # get user choice\n",
    "        choice = self._choosebest(v1, v2, noise=self.noises[idx])\n",
    "        # reward if agree with prior, penalize if against prior\n",
    "        if prior == choice:\n",
    "            self.reputations[idx] += self.reward\n",
    "            self.selected[idx] += 1\n",
    "        else:\n",
    "            self.reputations[idx] -= self.reward\n",
    "            self.selected[idx] += 1\n",
    "        return choice\n",
    "    \n",
    "    def getreputations(self):\n",
    "        return self.noises, self.reputations, self.selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Crowdscorer([0.25, 0.5, 0.66])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6675 6709 1 True\n",
      "6674 6712 0 False\n",
      "6664 6645 0 True\n",
      "6595 6676 1 True\n",
      "6648 6713 1 True\n",
      "6595 6676 1 True\n",
      "6644 6592 1 False\n",
      "6699 6700 0 False\n",
      "6596 6717 1 True\n",
      "6705 6693 1 False\n"
     ]
    }
   ],
   "source": [
    "# test the class version, with multiple users\n",
    "for i in range(10):\n",
    "    v1, v2 = getpairing(hits, elos) # < this is 'wrong' for real task, just testing\n",
    "    idx = test.score(v1, v2)\n",
    "    choices = [v1, v2]\n",
    "    if choices.index(max(choices)) == idx:\n",
    "        tru = \"True\"\n",
    "    else:\n",
    "        tru = \"False\"\n",
    "    print(v1, v2, idx, tru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update Elo function\n",
    "# this should be Elo scores, and index of winner\n",
    "# k is a parameter that can be graded (higher for newbies)\n",
    "# we set it relatively high, bc only ranking least-seen elements\n",
    "# PS yes it is 'Elo' not 'ELO' but i like electric light orchestra, what can i say\n",
    "def updateELO(elo1, elo2, win, k=32):\n",
    "    # win probabilities\n",
    "    e1prob = 1.0/(10**((elo2-elo1)/400.)+1)\n",
    "    e2prob = 1.0/(10**((elo1-elo2)/400.)+1)\n",
    "    # score update - flip it for first pos\n",
    "    e1scor = abs(1-win)\n",
    "    e2scor = win\n",
    "    # rating update\n",
    "    e1new = elo1 + (k*(e1scor-e1prob))\n",
    "    e2new = elo2 + (k*(e2scor-e2prob))\n",
    "    return e1new, e2new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1616.0, 1584.0)\n",
      "(1584.0, 1616.0)\n",
      "(2400.3168316831684, 1599.6831683168316)\n",
      "(2368.3168316831684, 1631.6831683168316)\n",
      "(1631.6831683168316, 2368.3168316831684)\n",
      "(1599.6831683168316, 2400.3168316831684)\n"
     ]
    }
   ],
   "source": [
    "# test some cases\n",
    "print(updateELO(1600, 1600, 0)) # even newbie, A wins\n",
    "print(updateELO(1600, 1600, 1)) # even newbie, B wins\n",
    "print(updateELO(2400, 1600, 0)) # better player wins\n",
    "print(updateELO(2400, 1600, 1)) # upset: newbie wins\n",
    "print(updateELO(1600, 2400, 0)) # rev: upset: newbie wins\n",
    "print(updateELO(1600, 2400, 1)) # rev: better player wins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iterative scoring\n",
    "\n",
    "while number of trials not reached:\n",
    "1. get a pairing according to the pairing policy\n",
    "2. have somewhat-unreliable scorer score the pair\n",
    "3. update the ELO scores and increment hit count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crowdscoring(data, elos, hits, users=10, n=10000, k=32):\n",
    "    # copy new instances of data\n",
    "    t_data = data[:]\n",
    "    t_elos = elos[:]\n",
    "    t_hits = hits[:]\n",
    "    # initialize scorers\n",
    "    noises  = [np.random.random()/1.5 for user in range(users)]\n",
    "    scorer = Crowdscorer(noises, reward=0.005, temperature=6)\n",
    "    # for each trial...\n",
    "    c = 1\n",
    "    for trial in range(n):\n",
    "        # get INDEX pairing\n",
    "        idx1, idx2 = getpairing(t_hits, t_elos)\n",
    "        # pass VALUES to \"user\" to score\n",
    "        win_idx = scorer.score(t_data[idx1], t_data[idx2])\n",
    "        # update hit count upon score receipt\n",
    "        t_hits[idx1] += 1\n",
    "        t_hits[idx2] += 1\n",
    "        # update ELOs\n",
    "        new1, new2 = updateELO(t_elos[idx1], t_elos[idx2], win_idx, k=k)\n",
    "        t_elos[idx1] = new1\n",
    "        t_elos[idx2] = new2\n",
    "        if trial > 0 and (trial+1) % int(n/10) == 0:\n",
    "            print((trial+1), \"trials done:\", c*10, '%...')\n",
    "            c += 1\n",
    "    return t_data, t_elos, t_hits, scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 trials done: 10 %...\n",
      "20000 trials done: 20 %...\n",
      "30000 trials done: 30 %...\n",
      "40000 trials done: 40 %...\n",
      "50000 trials done: 50 %...\n",
      "60000 trials done: 60 %...\n",
      "70000 trials done: 70 %...\n",
      "80000 trials done: 80 %...\n",
      "90000 trials done: 90 %...\n",
      "100000 trials done: 100 %...\n"
     ]
    }
   ],
   "source": [
    "end_data, end_elos, end_hits, scorer = crowdscoring(data, elos, hits, users=50, n=100000, k=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"final\" scores will be ranked according to ELO\n",
    "\n",
    "...though degree of success relies on number of trial, user reliability, *k* parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top ranked:\n",
      "elo \t value\n",
      "2061 \t  49714\n",
      "2033 \t  49687\n",
      "2023 \t  48429\n",
      "2017 \t  49726\n",
      "2015 \t  49858\n",
      "2011 \t  49385\n",
      "2006 \t  49724\n",
      "1994 \t  49623\n",
      "1991 \t  49527\n",
      "1983 \t  49245\n",
      "\n",
      "lowest ranked:\n",
      "elo \t value\n",
      "1214 \t   372\n",
      "1210 \t   311\n",
      "1204 \t   141\n",
      "1189 \t   444\n",
      "1185 \t    48\n",
      "1184 \t   228\n",
      "1181 \t    79\n",
      "1173 \t   457\n",
      "1160 \t   328\n",
      "1139 \t   744\n"
     ]
    }
   ],
   "source": [
    "# top scores\n",
    "srt_data, srt_elos = rank_x_by_y(end_data, end_elos)\n",
    "print(\"top ranked:\")\n",
    "print(\"elo\", '\\t', \"value\")\n",
    "for i in range(10):\n",
    "    print('{:04.0f}'.format(srt_elos[i]), '\\t', '{: 5.0f}'.format(srt_data[i]))\n",
    "print(\"\\nlowest ranked:\")\n",
    "print(\"elo\", '\\t', \"value\")\n",
    "for i in reversed(range(10)):\n",
    "    print('{:04.0f}'.format(srt_elos[-i-1]), '\\t', '{: 5.0f}'.format(srt_data[-i-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## examine whether expected choice is reliable reputation factor\n",
    "\n",
    "using expected outcome could be a problem with many new data, as new data is initialized at a 'neutral' score, so the 'expected' selections may be counter to the true answers?\n",
    "\n",
    "but in this toy demo, it appears that this is an acceptable scoring method, as reputation and times chosen correlate to reliability, and (with proper temperature adjustment), the selection probability does not _entirely_ shut out trolling users, while still favoring reliable scorers. this does have the chance to converge into a vicious cycle rewarding early adopters by favoring those with higher reputation. so this also needs to be taken into account.\n",
    "\n",
    "of course, in a real system, the 'reliability' is subjective and is a latent variable; it is only because of this task's artificiality that we can objectively peek at the \"true\" underlying reliability. and due to the fact that reputation will fluctuate, some smoothing factor should be implemented so that users are not constantly wondering why their reputation is going up and down.\n",
    "\n",
    "also, notice that this took ten times as many validations as samples; reducing `n` demonstrates that fewer iterations do not dependably sort the items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, r, s = scorer.getreputations()\n",
    "n = [1-x for x in n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reliability\t\treputation\t\ttimes chosen\n",
      "0.9973648829006753 \t 32.58499999999822 \t 6347\n",
      "0.9967286113438104 \t 28.044999999998705 \t 5445\n",
      "0.9960696739674756 \t 71.82000000000717 \t 14274\n",
      "0.9745095134622639 \t 18.380000000000628 \t 3624\n",
      "0.9380588610845417 \t 14.0700000000008 \t 2948\n",
      "0.9260490602275694 \t 11.155000000000344 \t 2419\n",
      "0.9195511440530173 \t 11.885000000000458 \t 2553\n",
      "0.9120009073592603 \t 10.665000000000267 \t 2319\n",
      "0.9050991212695274 \t 10.195000000000194 \t 2317\n",
      "0.8964566116872507 \t 10.43000000000023 \t 2366\n",
      "0.8736832262721358 \t 8.934999999999997 \t 2157\n",
      "0.8552839029406051 \t 8.239999999999888 \t 2090\n",
      "0.8315932055004788 \t 8.414999999999916 \t 2147\n",
      "0.8286462955629535 \t 7.744999999999856 \t 2009\n",
      "0.8121199091966981 \t 6.314999999999887 \t 1819\n",
      "0.808346785761692 \t 6.494999999999883 \t 1801\n",
      "0.7627688571199326 \t 5.2449999999999095 \t 1585\n",
      "0.7470668273700631 \t 4.954999999999916 \t 1549\n",
      "0.7282694709446265 \t 4.579999999999924 \t 1530\n",
      "0.6986021648238361 \t 3.9799999999999365 \t 1578\n",
      "0.6948254552374578 \t 3.6849999999999428 \t 1447\n",
      "0.684653393358819 \t 3.7799999999999407 \t 1428\n",
      "0.6809094139040186 \t 3.8799999999999386 \t 1526\n",
      "0.6775655444898627 \t 3.394999999999949 \t 1473\n",
      "0.6752563294488372 \t 3.299999999999951 \t 1428\n",
      "0.6625733928642643 \t 3.574999999999945 \t 1481\n",
      "0.6254105800985907 \t 2.829999999999961 \t 1408\n",
      "0.607523366478049 \t 2.169999999999975 \t 1246\n",
      "0.6025889762694052 \t 2.3249999999999718 \t 1309\n",
      "0.6008930602203302 \t 2.259999999999973 \t 1314\n",
      "0.5777640167715394 \t 1.989999999999979 \t 1290\n",
      "0.5717920893371062 \t 1.5799999999999876 \t 1236\n",
      "0.5188210475189543 \t 1.2649999999999944 \t 1251\n",
      "0.5111202567364994 \t 1.3649999999999922 \t 1241\n",
      "0.4961739765625103 \t 0.9599999999999999 \t 1154\n",
      "0.4925391152747157 \t 1.2449999999999948 \t 1225\n",
      "0.48741357826772624 \t 0.6199999999999996 \t 1144\n",
      "0.48720953595033956 \t 0.8199999999999997 \t 1160\n",
      "0.4736733988458851 \t 0.7349999999999997 \t 1197\n",
      "0.47282491871550214 \t 0.9099999999999998 \t 1188\n",
      "0.47163237238502376 \t 0.7649999999999997 \t 1157\n",
      "0.4702739061703076 \t 0.5049999999999994 \t 1185\n",
      "0.46817646328453333 \t 0.6549999999999996 \t 1157\n",
      "0.44924001878241815 \t 0.37499999999999933 \t 1095\n",
      "0.40612059469904394 \t -0.295000000000001 \t 1053\n",
      "0.40568614320629215 \t -0.03000000000000082 \t 1106\n",
      "0.3901031283835522 \t -0.160000000000001 \t 1046\n",
      "0.37616620016316793 \t -0.35500000000000104 \t 1067\n",
      "0.3441435857754902 \t -0.5650000000000013 \t 1047\n",
      "0.34381261662112184 \t -0.6200000000000012 \t 1064\n"
     ]
    }
   ],
   "source": [
    "print('reliability\\t\\treputation\\t\\ttimes chosen')\n",
    "for t in sorted(list(zip(n, r, s)), key=lambda x:x[0], reverse=True):\n",
    "    print(t[0], '\\t', t[1], '\\t', t[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
